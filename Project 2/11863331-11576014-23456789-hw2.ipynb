{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 24.52208709716797 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'51': [5872, 3066], '52': [54, 846, 2208], '53': [4886, 48553], '54': [2231, 1339, 1490], '55': [4400, 235], '56': [389, 4081, 241, 2294, 190739], '57': [155285], '58': [3454, 1920], '59': [804, 955, 90480], '60': [157422, 213, 215481], '61': [358, 523, 335, 1098, 2598], '62': [55, 64853, 73364], '63': [2129, 243152], '64': [1987, 451], '65': [281, 202871, 982], '66': [1535, 1434, 3628], '67': [3698, 5802, 416, 76611], '68': [248, 111208, 1322, 74181, 91961], '69': [2354, 203193, 3177, 811, 942], '70': [233033, 163926], '71': [580, 120650], '72': [71970, 218232, 5465, 276], '73': [71970, 218232, 21, 43984], '74': [61863, 238], '75': [30473], '76': [5465, 276, 1633, 1704, 3895], '77': [188098], '78': [105220], '79': [96348, 78, 33, 1801], '80': [214, 252, 789, 187515], '81': [380, 66582, 237310, 3367, 3383, 1893], '82': [5967, 3067], '83': [1122, 1189, 2783], '84': [2563, 201208, 601, 351, 851, 122153], '85': [85, 2181], '86': [140, 89553], '87': [819, 1533, 485, 629, 380, 1644], '88': [1912, 156, 255, 5609], '89': [78639, 3027, 2636, 219, 13], '90': [1275, 5810, 3564, 156, 1535, 694, 1821], '91': [5465, 276, 166, 2793, 2121, 341, 982], '92': [142, 55, 851, 205], '93': [3269, 21, 4040, 390], '94': [774, 20692, 1033], '95': [774, 20692, 1033, 73379], '96': [774, 20692, 447, 74122], '97': [91954, 176407, 3655], '98': [91954, 176407, 851, 2324], '99': [335, 1098, 2598], '100': [63065, 3063, 68, 1178], '101': [2569, 829, 2697, 274, 1373, 107, 191], '102': [140391, 420, 26337, 5465, 276, 276, 2088, 107, 2509], '103': [2845, 917], '104': [5630, 248, 746], '105': [196, 39], '106': [5465, 276, 203, 4400, 235], '107': [436, 3594, 4400, 235], '108': [436, 192943, 1122], '109': [483, 121844, 258], '110': [196, 2097, 54, 846, 8], '111': [313, 192484], '112': [2621, 40238], '113': [1, 496, 2231, 3655], '114': [392, 881, 2231, 140799], '115': [1404, 229, 2673, 77], '116': [100114, 94, 231164], '117': [2409, 5465, 276, 53245, 431, 1070], '118': [142, 3676], '119': [1533, 142, 3676], '120': [128, 1404, 142, 2199], '121': [168, 1042], '122': [198109, 1302, 1, 1042, 520, 625], '123': [420, 203, 51043], '124': [5814, 1982, 1042, 238862], '125': [274, 2336, 1533, 8], '126': [447, 1906, 2510, 1178], '127': [5465, 276, 5465, 276, 276, 377, 518, 203, 2527], '128': [191898, 11, 1099], '129': [19, 226762, 5465, 276], '130': [1056, 5581, 5465, 276, 250383, 515], '131': [154987, 2549, 1490, 55, 735], '132': [228094, 735], '133': [116975, 496, 237281], '134': [397, 100217, 812], '135': [391, 2911, 4104, 151493, 2619], '136': [76733, 1161, 237289], '137': [2856, 5465, 276, 3558, 719, 277], '138': [776, 163, 2296, 1987, 235195], '139': [2485, 1861, 1959, 1145, 69, 609, 4916], '140': [78, 1404, 1861, 97318], '141': [2313, 3069, 152, 3485, 5465, 276], '142': [1404, 8, 200099, 1510, 5030, 142, 515], '143': [1189, 5465, 276, 824], '144': [590, 269, 20, 92], '145': [1608, 862, 325, 5506], '146': [2731, 138, 1760, 416, 82], '147': [5442, 5609, 5465, 276, 395], '148': [1891, 116012, 690], '149': [985, 86670], '150': [5465, 276, 78, 118, 2387], '151': [63508, 178428, 4675], '152': [5197, 54952, 3576, 5465, 276, 107, 1709], '153': [746, 2230, 5189, 80, 373, 472], '154': [156, 226043], '155': [180, 1605, 832, 97318, 5465, 276], '156': [484, 84387, 1266, 203, 642], '157': [2743, 243556, 4755, 213404], '158': [373, 144171, 72, 5465, 276, 110], '159': [1812, 307, 519], '160': [254101, 67300, 659, 397, 20750], '161': [4072, 654], '162': [4735, 6153], '163': [922, 1931, 1583, 2897], '164': [100114, 625, 1000, 1240, 2324], '165': [2460, 38, 1822, 382], '166': [227473, 1449, 2587, 39370], '167': [3594, 1525, 684, 88519, 1856, 2127, 1847, 5723, 236, 2053, 52100], '168': [2387, 23989], '169': [388, 4451, 5552, 4193], '170': [4916, 119910, 220439, 99893, 45097, 3516], '171': [3076, 738, 2540, 2760, 1842], '172': [82174, 447, 588, 955, 463, 250488, 53672, 2336], '173': [2336, 5902], '174': [4204, 1664, 2643], '175': [172647, 5448, 1266, 203, 642], '176': [448, 170, 360, 1138], '177': [1424, 85, 1434, 5465, 276], '178': [2832, 153960], '179': [5465, 276, 3649, 69, 3173], '180': [121054, 5465, 276, 83861, 2208], '181': [1848, 2439, 186, 72, 447, 171612, 1466, 5279, 303, 159894, 160909], '182': [881, 178525, 65624, 328, 1781, 692], '183': [28392, 955, 3924], '184': [1329, 3614, 310, 738], '185': [917, 5465, 276, 2845, 191], '186': [2353, 3672, 1116, 4934, 35, 2791, 114, 192], '187': [1459, 71869, 631, 2846], '188': [36062, 86216], '189': [448, 163974, 781], '190': [122164, 1160, 1483, 774], '191': [484, 1723, 5465, 276, 212881], '192': [156, 2177, 2643], '193': [5098, 377, 2178], '194': [946, 97, 2152, 2642], '195': [75, 57, 29891, 774, 121675, 235], '196': [114, 1268, 254885, 191, 2183, 1622, 5465, 276, 3579, 119], '197': [917, 128371, 191, 539, 128358, 104782, 249163, 3291, 2371], '198': [4104, 238866, 1312, 117322], '199': [141966, 156066, 29064, 231615], '200': [1404, 69, 238359, 1783, 5465, 276, 238359, 277]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TREC run files\n",
    "\n",
    "Function <font color='blue'>run_retrieval</font> runs a retrieval method for all the topic queries and stores the TREC-friendly results in a file (<font color='blue'>model_name.run</font>). It is generic, since it handles any of the implemented score functions of Task 1 (except PLM scoring) in the same way. The score of each topic query for a document is the sum of the scores of each topic query term upon the document.\n",
    "\n",
    "**Note:** If a run filename already exists, then it returns immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for doc in range(index.document_base(), num_documents+1):\n",
    "        # Skip empty documents\n",
    "        if index.document_length(doc) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get document id\n",
    "        ext_doc_id = index.document(doc)[0]\n",
    "        \n",
    "        # Iterate over topic queries\n",
    "        for query_id in tokenized_queries.keys():\n",
    "            if query_id not in data.keys():\n",
    "                data[query_id] = []\n",
    "            score = 0   \n",
    "            # Iterate over terms of topic query and calculate score\n",
    "            for query_term_id in tokenized_queries[query_id]:\n",
    "                if doc not in inverted_index[query_term_id]:\n",
    "                    document_term_freq = 0\n",
    "                else:\n",
    "                    document_term_freq = inverted_index[query_term_id][doc]\n",
    "                score += score_fn(doc, query_term_id, document_term_freq)\n",
    "            data[query_id].append((score, ext_doc_id))\n",
    "    \n",
    "    # Transform to tuples\n",
    "    for query_id in data.keys():\n",
    "        data[query_id] = tuple(data[query_id])\n",
    "    \n",
    "    # Save to file\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    \n",
    "    print('Retrieved in {0} seconds'.format(time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score methods and results\n",
    "The required score methods are implemented here. After the implementation, the steps followed for each method were:\n",
    "\n",
    "1. Creation of the run files. If a score method has hyperameters that are asked to be optimized, then different run files are created for each of them. For instance, in Jelinek-Mercer we are asked to optimize the interpolation hyperparameter $\\lambda$ within the range of values [0.1, 0.5, 0.9]. This allows us to evaluate the scoring of each run file upon the validation set and choose the best of them.\n",
    "\n",
    "2. **(if applicable)** Get the evaluation results (NDCG@10, M.A.P.@1000, Precision@5, Recall@1000) for different hyperparameters of each model upon the <em>validation set</em>. Choose the one with the best score in NDCG@10.\n",
    "\n",
    "3. Get the evaluation results (same as above) of each model upon the <em>test set</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function <font color='blue'>evaluate</font> calculates NDCG@10, MAP@1000, Precision@5, Recall@1000 evaluation measures of a given run file upon a evaluation set.  This is done by invoking a trec_eval bash script (<font color='blue'>run_trec_validation.sh</font>). The results are stored in a dictionary structure:\n",
    "\n",
    "{'ndcg_cut_10' : {query1 : score1, ..., 'all' : score}, 'map_cut_1000' : {query1 : score1, ..., 'all' : score},\n",
    "'P_5' : {query1 : score1, ..., 'all' : score}, 'recall_1000' : {query1 : score1, ..., 'all' : score}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(evaluation_set, run_model):\n",
    "    \"\"\" Invokes trec and returns all the results on NDCG@10, MAP@1000, Precision@5, Recall@1000.\n",
    "    \n",
    "    Key arguments:\n",
    "    evaluation_set -- The name of the qrel file.\n",
    "    run_model -- The name of the run file.\n",
    "    \"\"\"\n",
    "    # Set up the trec script and direct the output to a pipe\n",
    "    run_model += \".run\"\n",
    "    p1 = subprocess.Popen(['./run_trec_validation.sh', evaluation_set, run_model], stdout=subprocess.PIPE)\n",
    "    \n",
    "    # Run the command\n",
    "    output = p1.communicate()[0]\n",
    "    \n",
    "    # Save the results\n",
    "    results = {}\n",
    "    for line in output.decode('utf8').splitlines():\n",
    "        tokens = line.split()\n",
    "        if tokens[0] not in results:\n",
    "            results[tokens[0]] = {}\n",
    "        results[tokens[0]][tokens[1]] = tokens[2]\n",
    "    return results\n",
    "\n",
    "validation_set = \"./ap_88_89/qrel_validation\"\n",
    "test_set = \"./ap_88_89/qrel_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-IDF\n",
    "\n",
    "TFIDF weight of a term upon a document d is the product of its td weight and its idf weight:\n",
    "$$tfidf(t;d) = log(1+tf(t;d))\\log{\\frac{n}{df(t)}}$$\n",
    "\n",
    ", n is the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    TF-IDF scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "    df = len(inverted_index[query_term_id]) \n",
    "    idf = np.log(num_documents/df)\n",
    "    score = np.log(1+document_term_freq) * idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF: Results upon the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF has the following scores upon the test set:\n",
      "nDCG@10:\t0.4169\n",
      "MAP@1000:\t0.2155\n",
      "Precision@5:\t0.4317\n",
      "Recall@1000:\t0.6510\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('tfidf', tfidf)\n",
    "TFIDF_results = evaluate(test_set, \"tfidf\")\n",
    "print(\"TF-IDF has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(TFIDF_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(TFIDF_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(TFIDF_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(TFIDF_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BM25 with k1=1.2 and b=0.75\n",
    "\n",
    "BM25 estimates a score of a document with the following:\n",
    "\n",
    "$$BM25 = \\sum_{t\\in q}{\\frac {(k_1+1)tf_{d,t}} {k_1((1-b) + b(\\frac{l_d}{l_{avg}})) + tf_{d,t}} idf(t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BM25(int_document_id, query_term_id, document_term_freq, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Calculate remaining parameters:\n",
    "    df = len(inverted_index[query_term_id]) \n",
    "    idf = np.log(num_documents/df)\n",
    "    ld = document_lengths[int_document_id]\n",
    "    lavg = avg_doc_length\n",
    "    \n",
    "    # Calculate final score:\n",
    "    numerator = (k1+1)*document_term_freq*idf\n",
    "    nominator = k1*((1-b) + b*(ld/lavg)) + document_term_freq\n",
    "    score = numerator/nominator\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25: Results upon the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 has the following scores upon the test set:\n",
      "nDCG@10:\t0.4086\n",
      "MAP@1000:\t0.2173\n",
      "Precision@5:\t0.4133\n",
      "Recall@1000:\t0.6524\n"
     ]
    }
   ],
   "source": [
    "run_retrieval('bm25', BM25)\n",
    "BM25_results = evaluate(test_set, \"bm25\")\n",
    "print(\"BM25 has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(BM25_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(BM25_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(BM25_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(BM25_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Models\n",
    "\n",
    "### 3.1 Jelinek-Mercer\n",
    "\n",
    "Jelinek-Mercer is a Multinomial Smoothing method that estimates the score for a term $w$ given a document $d$ by using interpolation. Julinek-Mercer probability of a given term and document is estimated as follows:\n",
    "$$\\hat{P_{\\lambda}}(w\\mid d) = \\lambda\\frac{tf(w;d)}{\\mid d\\mid} + (1-\\lambda)\\frac{tf(w;C)}{\\mid C\\mid}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JelinekMercer(int_document_id, query_term_id, document_term_freq, lambda_param=0.5):\n",
    "    \"\"\"\n",
    "    Jelinek-Mercer scoring function for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term\n",
    "    :param lambda_param: the lambda parameter used in the calculation of Jelinek-Mercer score\n",
    "    \"\"\"\n",
    "    # Get remaining parameters\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    collection_term_freq = collection_frequencies[query_term_id]\n",
    "    \n",
    "    # Calculate (log) score\n",
    "    score = np.log(lambda_param*(document_term_freq/doc_length) + \\\n",
    "                   (1-lambda_param)*(collection_term_freq/total_terms))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek-Mercer: Optimization of hyperparameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jelinek-Mercer with λ=0.1 scored 0.3991 for nDCG@10.\n",
      "Jelinek-Mercer with λ=0.5 scored 0.3823 for nDCG@10.\n",
      "Jelinek-Mercer with λ=0.9 scored 0.3676 for nDCG@10.\n",
      "Best scored achieved for λ = 0.1.\n"
     ]
    }
   ],
   "source": [
    "# Get run file for λ=0.1\n",
    "JelinekMercer01 = partial(JelinekMercer, lambda_param = 0.1)\n",
    "run_retrieval('JelinekMercer01', JelinekMercer01)\n",
    "\n",
    "# Get run file for λ=0.5\n",
    "JelinekMercer05 = partial(JelinekMercer, lambda_param = 0.5)\n",
    "run_retrieval('JelinekMercer05', JelinekMercer05)\n",
    "\n",
    "# Get run file for λ=0.9\n",
    "JelinekMercer09 = partial(JelinekMercer, lambda_param = 0.9)\n",
    "run_retrieval('JelinekMercer09', JelinekMercer09)\n",
    "\n",
    "# Get all evaluation results.\n",
    "JM_results_01 = evaluate(validation_set, \"JelinekMercer01\")\n",
    "JM_results_05 = evaluate(validation_set, \"JelinekMercer05\")\n",
    "JM_results_09 = evaluate(validation_set, \"JelinekMercer09\")\n",
    "\n",
    "# Choose the best hyperparameter according to the scores of NDCG@10\n",
    "print(\"Jelinek-Mercer with λ=0.1 scored {0} for nDCG@10.\".format(JM_results_01['ndcg_cut_10']['all']))\n",
    "print(\"Jelinek-Mercer with λ=0.5 scored {0} for nDCG@10.\".format(JM_results_05['ndcg_cut_10']['all']))\n",
    "print(\"Jelinek-Mercer with λ=0.9 scored {0} for nDCG@10.\".format(JM_results_09['ndcg_cut_10']['all']))\n",
    "print(\"Best scored achieved for λ = 0.1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek-Mercer: Results upon the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jelinek-Mercer with λ=0.1 has the following scores upon the test set:\n",
      "nDCG@10:\t0.3489\n",
      "MAP@1000:\t0.1893\n",
      "Precision@5:\t0.3450\n",
      "Recall@1000:\t0.6203\n"
     ]
    }
   ],
   "source": [
    "JM_results = evaluate(test_set, \"JelinekMercer01\")\n",
    "print(\"Jelinek-Mercer with λ=0.1 has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(JM_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(JM_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(JM_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(JM_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dirichlet Prior\n",
    "\n",
    "Dirichlet Prior Smoothing for a term given a document is estimated by the following equation:\n",
    "$$p(w\\mid \\hat{\\theta_d}) = \\frac{tf(w;d) + \\mu p(w\\mid C)}{\\mid d\\mid + \\mu}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DirichletPrior(int_document_id, query_term_id, document_term_freq, mu=1000):\n",
    "    \"\"\"\n",
    "    Dirichlet Prior scoring function for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term\n",
    "    :param lambda_param: the lambda parameter used in the calculation of Jelinek-Mercer score\n",
    "    \"\"\"\n",
    "    # Get remaining parameters\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    collection_term_prob = collection_frequencies[query_term_id]/total_terms\n",
    "    \n",
    "    # Calculate (log) score\n",
    "    score = np.log((document_term_freq + mu*collection_term_prob)/(doc_length+mu))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlet Prior Smoothing: Optimization of hyperparameter $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirichlet Prior with μ=500 scored 0.4055 for nDCG@10.\n",
      "Dirichlet Prior with μ=1000 scored 0.4002 for nDCG@10.\n",
      "Dirichlet Prior with μ=1500 scored 0.4026 for nDCG@10.\n",
      "Best scored achieved for μ = 500.\n"
     ]
    }
   ],
   "source": [
    "# Get run file for μ=500\n",
    "DirichletPrior500 = partial(DirichletPrior, mu=500)\n",
    "run_retrieval('DirichletPrior500', DirichletPrior500)\n",
    "\n",
    "# Get run file for μ=1000\n",
    "DirichletPrior1000 = partial(DirichletPrior, mu=1000)\n",
    "run_retrieval('DirichletPrior1000', DirichletPrior1000)\n",
    "\n",
    "# Get run file for μ=1500\n",
    "DirichletPrior1500 = partial(DirichletPrior, mu=1500)\n",
    "run_retrieval('DirichletPrior1500', DirichletPrior1500)\n",
    "\n",
    "# Get all evaluation results.\n",
    "DP_results_500 = evaluate(validation_set, \"DirichletPrior500\")\n",
    "DP_results_1000 = evaluate(validation_set, \"DirichletPrior1000\")\n",
    "DP_results_1500 = evaluate(validation_set, \"DirichletPrior1500\")\n",
    "\n",
    "# Choose the best hyperparameter according to the scores of NDCG@10\n",
    "print(\"Dirichlet Prior with μ=500 scored {0} for nDCG@10.\".format(DP_results_500['ndcg_cut_10']['all']))\n",
    "print(\"Dirichlet Prior with μ=1000 scored {0} for nDCG@10.\".format(DP_results_1000['ndcg_cut_10']['all']))\n",
    "print(\"Dirichlet Prior with μ=1500 scored {0} for nDCG@10.\".format(DP_results_1500['ndcg_cut_10']['all']))\n",
    "print(\"Best scored achieved for μ = 500.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlet Prior Smoothing: Results upon the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirichlet Prior with μ=500 has the following scores upon the test set:\n",
      "nDCG@10:\t0.4085\n",
      "MAP@1000:\t0.2104\n",
      "Precision@5:\t0.4200\n",
      "Recall@1000:\t0.6287\n"
     ]
    }
   ],
   "source": [
    "DP_results = evaluate(test_set, \"DirichletPrior500\")\n",
    "print(\"Dirichlet Prior with μ=500 has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(DP_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(DP_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(DP_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(DP_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Absolute discounting\n",
    "\n",
    "Absolute Discounting score is estimated by the following:\n",
    "\n",
    "$$p_{\\delta}(w\\mid \\hat{\\theta_d}) = \\frac{max(tf(w;d) - \\delta, 0)}{\\mid d\\mid} + \\frac{\\delta\\mid d\\mid_u}{\\mid d\\mid}p(w\\mid C)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AbsoluteDiscounting(int_document_id, query_term_id, document_term_freq, delta=0.5):\n",
    "    \"\"\"\n",
    "    Absolute Discounting scoring function for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term\n",
    "    :param lambda_param: the lambda parameter used in the calculation of Jelinek-Mercer score\n",
    "    \"\"\"\n",
    "    # Get remaining parameters\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    doc_voc = unique_terms_per_document[int_document_id]\n",
    "    collection_term_prob = collection_frequencies[query_term_id]/total_terms\n",
    "    \n",
    "    # Calculate (log) score\n",
    "    score = np.log(max((document_term_freq-delta), 0)/doc_length + (delta*doc_voc*collection_term_prob)/doc_length)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Discounting: Optimization of hyperparameter $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Discounting with δ=0.1 scored 0.3614 for nDCG@10.\n",
      "Absolute Discounting with δ=0.5 scored 0.3768 for nDCG@10.\n",
      "Absolute Discounting with δ=0.9 scored 0.3950 for nDCG@10.\n",
      "Best scored achieved for δ = 0.9.\n"
     ]
    }
   ],
   "source": [
    "# Get run file for δ=0.1\n",
    "AbsoluteDiscounting01 = partial(AbsoluteDiscounting, delta = 0.1)\n",
    "run_retrieval('AbsoluteDiscounting01', AbsoluteDiscounting01)\n",
    "\n",
    "# Get run file for δ=0.5\n",
    "AbsoluteDiscounting05 = partial(AbsoluteDiscounting, delta = 0.5)\n",
    "run_retrieval('AbsoluteDiscounting05', AbsoluteDiscounting05)\n",
    "\n",
    "# Get run file for δ=0.9\n",
    "AbsoluteDiscounting09 = partial(AbsoluteDiscounting, delta = 0.9)\n",
    "run_retrieval('AbsoluteDiscounting09', AbsoluteDiscounting09)\n",
    "\n",
    "# Get all evaluation results.\n",
    "AD_results_01 = evaluate(validation_set, \"AbsoluteDiscounting01\")\n",
    "AD_results_05 = evaluate(validation_set, \"AbsoluteDiscounting05\")\n",
    "AD_results_09 = evaluate(validation_set, \"AbsoluteDiscounting09\")\n",
    "\n",
    "# Choose the best hyperparameter according to the scores of NDCG@10\n",
    "print(\"Absolute Discounting with δ=0.1 scored {0} for nDCG@10.\".format(AD_results_01['ndcg_cut_10']['all']))\n",
    "print(\"Absolute Discounting with δ=0.5 scored {0} for nDCG@10.\".format(AD_results_05['ndcg_cut_10']['all']))\n",
    "print(\"Absolute Discounting with δ=0.9 scored {0} for nDCG@10.\".format(AD_results_09['ndcg_cut_10']['all']))\n",
    "print(\"Best scored achieved for δ = 0.9.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Discounting: Results upon the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Discounting with δ=0.9 has the following scores upon the test set:\n",
      "nDCG@10:\t0.3860\n",
      "MAP@1000:\t0.2031\n",
      "Precision@5:\t0.3983\n",
      "Recall@1000:\t0.6262\n"
     ]
    }
   ],
   "source": [
    "AD_results = evaluate(test_set, \"AbsoluteDiscounting09\")\n",
    "print(\"Absolute Discounting with δ=0.9 has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(AD_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(AD_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(AD_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(AD_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Positional Language Models (σ=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are given two positions i and j and a σ value and calculate the propagated count to position i from a term at position j $w_j$. Five different kernelsare used to calculate the propagated counts:\n",
    "\n",
    "1. Gaussian kernel\n",
    "2. Triangle kernel\n",
    "3. Cosine (Hamming) kernel\n",
    "4. Circle kernel\n",
    "5. Passage kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(i,j,sigma=50):\n",
    "    \"\"\"Returns the propagated count using Gaussian kernel\"\"\"\n",
    "    return np.exp(-((i-j)**2)/ (2*(sigma**2)))\n",
    "    \n",
    "def triangle_kernel(i,j,sigma=50):\n",
    "    \"\"\"Returns the propagated count using Triangle kernel\"\"\"\n",
    "    return 1-(math.fabs(i-j)/sigma) if math.fabs(i-j) <= sigma else 0\n",
    "    \n",
    "def cosine_kernel(i,j,sigma=50):\n",
    "    \"\"\"Returns the propagated count using Cosine kernel\"\"\"\n",
    "    return 0.5*(1+ math.cos((math.fabs(i-j)*math.pi)/sigma)) if math.fabs(i-j) <= sigma else 0\n",
    "    \n",
    "def circle_kernel(i,j,sigma=50):\n",
    "    \"\"\"Returns the propagated count using Circle kernel\"\"\"\n",
    "    return math.sqrt(1-(math.fabs(i-j)/sigma)**2) if math.fabs(i-j) <= sigma else 0\n",
    "\n",
    "def passage_kernel(i,j,sigma=50):\n",
    "    \"\"\"Returns the propagated count using Passage kernel\"\"\"\n",
    "    return 1 if math.fabs(i-j) <= sigma else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Cumulative Distribution Functions\n",
    "\n",
    "Since the calculation of the propagation count sums is computationally expensive, we decided to calculate them using the proposed solution from [Positional Language Models for Information Retrieval](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) which uses the Cumulative Distribution Functions of each kernel. This makes the calculation of $Z_i$'s much faster. The implementation follows the [C++ code](http://sifaka.cs.uiuc.edu/~ylv2/pub/plm/PLMRetEval.cpp) implemented by the authors of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GaussianCDF(x, mean, sigma=50):\n",
    "    \"\"\"The Gaussian CDF\"\"\"\n",
    "    x = (x-mean)/sigma\n",
    "    if x == 0:\n",
    "        res = 0.5\n",
    "    else:\n",
    "        oor2pi = 1/math.sqrt(2*math.pi)\n",
    "        t = 1/(1 + 0.2316419*math.fabs(x))\n",
    "        t *= oor2pi * math.exp(-0.5 * x * x) \\\n",
    "             * (0.31938153   + t  \\\n",
    "             * (-0.356563782 + t \\\n",
    "             * (1.781477937  + t  \\\n",
    "             * (-1.821255978 + t * 1.330274429))))\n",
    "        if x >= 0:\n",
    "            res = 1 - t\n",
    "        else:\n",
    "            res = t\n",
    "    return res\n",
    "\n",
    "def TriangleCDF(x, mean, sigma=50):\n",
    "    \"\"\"The Triangle CDF\"\"\"\n",
    "    x = (x-mean)/sigma\n",
    "    if x >= 1:\n",
    "        res = sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = (sigma*(1-math.fabs(x))*(1-math.fabs(x)))/2\n",
    "    else:\n",
    "        res = sigma - (sigma*(1-x)*(1-x))/2\n",
    "    return res\n",
    "\n",
    "def CosineCDF(x, mean, sigma=50):\n",
    "    \"\"\"The Cosine CDF\"\"\"\n",
    "    x = (x-mean)/sigma\n",
    "    if x >= 1:\n",
    "        res = sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = (sigma*(1 + x - math.sin(math.pi*x)/math.pi))/2\n",
    "    else:\n",
    "        res = sigma - (sigma*(1 - x + math.sin(math.pi*x)/math.pi))/2\n",
    "    return res\n",
    "\n",
    "\n",
    "def CircleCDF(x, mean, sigma=50):\n",
    "    \"\"\"The Circle CDF\"\"\"\n",
    "    x = (x-mean)/sigma\n",
    "    if x >= 1:\n",
    "        res = (math.pi/2)*sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma*(math.asin(x) + math.pi / 2 - math.sqrt(1-x*x))\n",
    "    else:\n",
    "        res = (math.pi - 2)*sigma - sigma*(math.asin(-x) + math.pi/2 - math.sqrt(1-x*x))\n",
    "    return res\n",
    "\n",
    "def PassageCDF():\n",
    "    \"\"\"The Passage CDF. (Note: there is no actual CDF for Passage kernel and a normalization method is followed)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function estimates the normalized length $Z_i$ of a document given:\n",
    "\n",
    "1. A position i in the document\n",
    "2. The length of the document\n",
    "3. A propagation function (CDF of a kernel)\n",
    "4. A σ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized_length_Zi(i, doc_length, prop_func, sigma=50):\n",
    "    \"\"\"Returns the normalized length of a document in a specific position.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    i -- a position in the document\n",
    "    doc_length -- the length of the document\n",
    "    prop_func -- a kernel CDF\n",
    "    sigma -- a sigma value\n",
    "    \"\"\"\n",
    "    Zi = 0\n",
    "    # Using Gaussian CDF\n",
    "    if prop_func == GaussianCDF:\n",
    "        Zi = math.sqrt(2*math.pi)*sigma*(prop_func(doc_length, i, sigma) - prop_func(0, i, sigma))\n",
    "    # Using Passage CDF\n",
    "    elif prop_func == PassageCDF:\n",
    "        if sigma > doc_length - i:\n",
    "            Zi += doc_length - i\n",
    "        else:\n",
    "            Zi += sigma\n",
    "        if(sigma > i):\n",
    "            Zi += i\n",
    "        else:\n",
    "            Zi += sigma + 1 if i > sigma else sigma\n",
    "    # All other kernel CDFs\n",
    "    else:\n",
    "        Zi = prop_func(doc_length, i, sigma) - prop_func(0, i, sigma)\n",
    "    return Zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper dictionary that maps a kernel name to its corresponding kernel and CDF functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_map = {\"gaussian\" : (gaussian_kernel, GaussianCDF),\n",
    "              \"triangle\" : (triangle_kernel, TriangleCDF),\n",
    "              \"cosine\"   : (cosine_kernel, CosineCDF), \n",
    "              \"circle\"   : (circle_kernel, CircleCDF),\n",
    "              \"passage\"  : (passage_kernel, PassageCDF)\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Tables (for efficiency)\n",
    "Since it is computationally expensive to compute the propagated counts during the retrieval, we decided to create all possible propagated count values beforehand and store them into a file **kernel_tables.pickle**. The following function does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_kernels():\n",
    "    \"\"\"Calculates all possible kernel operations upon all possible positions and stores them in a file.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print('Getting all kernel tables')\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    # Get the length of the largest document.\n",
    "    max_len = max([document_lengths[doc] for doc in range(index.document_base(), num_documents+1)])\n",
    "    \n",
    "    # Create a kernel table for all possible positions for all possible kernels\n",
    "    # kernel_table: name -> (i,j) -> float x, with i,j in [0, max_len-1] and x the score\n",
    "    kernel_tables = {\"gaussian\" : {}, \"triangle\" : {}, \"cosine\" : {}, \"circle\" : {}, \"passage\" : {}}\n",
    "    for i in range(max_len):\n",
    "        for j in range(max_len):\n",
    "            for kernel_name in kernel_tables.keys():\n",
    "                kernel_tables[kernel_name][(i,j)] = kernel_map[kernel_name][0](i,j)\n",
    "    \n",
    "    # Store table in a file using pickle library\n",
    "    p = open(\"kernel_tables.pickle\",\"wb\")\n",
    "    pickle.dump(kernel_tables, p)\n",
    "    print(\"Calculated kernel tables in {0} seconds.\".format(time.time() - start_time))\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If kernel_tables.pickle is not created then call store_kernels(). \n",
    "Then loading of the kernel table from the corresponding file is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If kernel_tables are not stored, then call store_kernels()\n",
    "if not os.path.exists(\"kernel_tables.pickle\"):\n",
    "    store_kernels()\n",
    "\n",
    "# Retrieve kernel tables\n",
    "p = open(\"kernel_tables.pickle\", 'rb')\n",
    "kernel_table = pickle.load(p)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized lengths $Z_i$ (for efficiency)\n",
    "For the same reason as with the propagated counts, we decided to calculate all possible normalized lengths $Z_i$'s before running the retrieval method. We produce all possible normalized lengths starting from the first position till the last position of the lengthest document. The structure is the saved in **Z.pickle** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_Zs():\n",
    "    \"\"\"Calculates all possible normalized lengths and stores them in a file.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print('Getting all normalized lengths')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get the length of the largest document.\n",
    "    max_len = max([document_lengths[doc] for doc in range(index.document_base(), num_documents+1)])\n",
    "    \n",
    "    # Create a normalized length vector dictionary for all possible positions for all possible kernels\n",
    "    # Z: name -> i -> float x, with i in [0, max_len-1] and x the value Zi\n",
    "    Z = {\"gaussian\" : {}, \"triangle\" : {}, \"cosine\" : {}, \"circle\" : {}, \"passage\" : {}}\n",
    "    for length in range(1,max_len+1):\n",
    "        for kernel_name in Z.keys():\n",
    "            Z[kernel_name][length] = []\n",
    "            for pos in range(length):\n",
    "                Z[kernel_name][length].append(normalized_length_Zi(pos, length, kernel_map[kernel_name][1]))\n",
    "    \n",
    "    # Store table in a file using pickle library\n",
    "    p = open(\"Z.pickle\",\"wb\")\n",
    "    pickle.dump(Z, p)\n",
    "    print(\"Calculated normalized lengths in {0} seconds.\".format(time.time() - start_time))\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Z.pickle is not created then call store_Zs(). \n",
    "Then loading of the normalized lengthes from the corresponding file is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If Z is not stored, then call store_Zs()\n",
    "if not os.path.exists(\"Z.pickle\"):\n",
    "    store_Zs()\n",
    "\n",
    "# Retrieve Z vectors\n",
    "p = open(\"Z.pickle\", 'rb')\n",
    "Z = pickle.load(p)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagated Count\n",
    "\n",
    "The propagated count is calculated with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagated_count(query_term_id, int_document_id, pos, kernel_name, positions):\n",
    "    occurrences = positions[query_term_id]\n",
    "    return sum([kernel_table[kernel_name][(pos, j)] for j in occurrences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlet smoothed PLM\n",
    "\n",
    "We decided to apply Dirichlet smoothing for the implementation of the PLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dirichlet_smoothed_PLM(query_term_id, int_document_id, pos, kernel_name, positions, mu=500):\n",
    "    # Propagated count\n",
    "    pc = propagated_count(query_term_id, int_document_id, pos, kernel_name, positions)\n",
    "    # General collection Language Model propability\n",
    "    collection_term_prob = collection_frequencies[query_term_id]/total_terms\n",
    "    # The length of the document\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    # The propagated count sum in position i\n",
    "    Zi = Z[kernel_name][doc_length][pos]\n",
    "    \n",
    "    return (pc+mu*collection_term_prob)/(Zi+mu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring at position i\n",
    "\n",
    "KL-Divergence is the metric that we use to estimate the score of a query upon a specific position of a document.\n",
    "Only the query terms contribute to the score, since other terms produce zero result to their contribution in the summation since the MLE of them is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KLdivergence(query_id, int_document_id, pos, kernel_name, positions, mu=500):\n",
    "    score = 0\n",
    "    query_len = len(tokenized_queries[query_id])\n",
    "    # Calculate the contribution of each query term to the score\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        # Maximum Likelihood Estimate\n",
    "        tf = len([t for t in tokenized_queries[query_id] if t == query_term_id])\n",
    "        pr = tf/query_len\n",
    "        # The Dirichlet smoothed PLM\n",
    "        plm = Dirichlet_smoothed_PLM(query_term_id, int_document_id, pos, kernel_name, positions, mu)\n",
    "        # Score summation\n",
    "        score += pr*np.log(pr/plm)\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best position stategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def best_position_strategy(query_id, int_document_id, positions, mu=500):\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    pos_scores = {\"gaussian\" : [], \"triangle\" : [], \"cosine\" : [], \"circle\" : [], \"passage\" : []}\n",
    "    \n",
    "    for kernel_name in pos_scores.keys():\n",
    "        for pos in range(doc_length):\n",
    "            pos_scores[kernel_name].append(KLdivergence(query_id, int_document_id, pos, kernel_name, positions, mu))\n",
    "        \n",
    "        pos_scores[kernel_name] = max(pos_scores[kernel_name])\n",
    "    return pos_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF reranking\n",
    "\n",
    "Since PLMs are very computationally expensive, the ranking of all the documents for each query needs a huge amount of time to be calculated. For that reason, we decided to rerank the top k documents of the results of the best performing ranking method. TF-IDF scored the highest value of NDCG@10, therefore we made the following restrictions:\n",
    "\n",
    "1. For the **hyperparameter optimization** upon the validation set we rerank the top **100** ranked documents of TF-IDF\n",
    "2. For the scoring, we use the best performing setting upon the validation set. We rerank the top **100** documents of TF-IDF\n",
    "\n",
    "The following function retrieves the top n documents of the TF-IDF results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_rankings(n=100):\n",
    "    \n",
    "    # Get a mapping from document names (e.g. \"AP880318-0287) to their dictionary IDs.\n",
    "    doc2id = {}\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        doc2id[index.document(int_doc_id)[0]] = int_doc_id\n",
    "        \n",
    "    # Get tfidf rankings\n",
    "    names = [\"qid\", \"Q0\", \"docid\", \"rank\", \"similarity\", \"run\"]\n",
    "    data = pd.read_csv('tfidf.run', sep=' ', header=None, names=names)\n",
    "    \n",
    "    #data = data[[\"qid\", \"docid\"]]\n",
    "    last_data = pd.DataFrame(columns=[\"qid\", \"docid\"])\n",
    "    final_data = pd.DataFrame(columns=[\"qid\", \"docid\"])\n",
    "    for query in tokenized_queries.keys():\n",
    "        temp = data.loc[data['qid'] == int(query)]\n",
    "        \n",
    "        # Take rest of rankings\n",
    "        rest = data.loc[data['qid'] == int(query)].tail(1000-n)\n",
    "        \n",
    "        # Add large negative values to similarity\n",
    "        rest[\"similarity\"] = range(-9999 ,-9999-rest[\"similarity\"].shape[0], -1)\n",
    "        \n",
    "        # Append to last rankings table\n",
    "        last_data = last_data.append(rest)\n",
    "        \n",
    "        # Get the top n rankings\n",
    "        final_data = final_data.append(temp.head(n))\n",
    "    \n",
    "    # Create a dictionary for the top n rankings:\n",
    "    # results: doc_id -> list of queries which have this doc in their top n rankings\n",
    "    final_data = final_data[[\"qid\", \"docid\"]]\n",
    "    results = defaultdict(list)\n",
    "    for _, row in final_data.iterrows():\n",
    "        results[doc2id[row[\"docid\"]]].append(str(row[\"qid\"]))\n",
    "        \n",
    "    return results, last_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_docs100, rest_ranking100 = get_tfidf_rankings(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary of position (for efficiency)\n",
    "\n",
    "The following function given a Document ID, returns a dictionary which maps each word appearing in the document to a list of positions. This is an important speed up, since the propagated counts need fast access to the positions of the query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_positions(docid):\n",
    "    positions = defaultdict(list)\n",
    "    for pos, word in enumerate([w for w in index.document(docid)[1] if w > 0]):\n",
    "        positions[word].append(pos)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Ranking with PLM\n",
    "We rerank the top-100 ranks of the results by TF-IDF. The results are stored in a run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_PLM(docs, rest, mu=500):\n",
    "    \n",
    "    path = \"plm_gaussian_\" + str(mu) + \".run\"\n",
    "    \n",
    "    # Don't run again if files exist\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    \n",
    "    data = { \"gaussian\" : defaultdict(list),\n",
    "             \"triangle\" : defaultdict(list),\n",
    "             \"cosine\"   : defaultdict(list),\n",
    "             \"circle\"   : defaultdict(list),\n",
    "             \"passage\"  : defaultdict(list)\n",
    "           }\n",
    "            \n",
    "    start_time = time.time()\n",
    "    \n",
    "    i= 1\n",
    "    for doc in docs.keys():\n",
    "        if i%1000 == 0:\n",
    "            print(\"Proccessed\", i, \"documents in\", time.time() - start_time, \"seconds\")\n",
    "        i += 1\n",
    "        \n",
    "        # Get document id\n",
    "        ext_doc_id = index.document(doc)[0]\n",
    "        \n",
    "        # Get all document word position (important speedup)\n",
    "        positions = get_word_positions(doc)\n",
    "        \n",
    "        # Iterate over topic queries\n",
    "        for query_id in docs[doc]:\n",
    "            score = best_position_strategy(query_id, doc, positions, mu)\n",
    "            data[\"gaussian\"][query_id].append((score[\"gaussian\"], ext_doc_id))\n",
    "            data[\"triangle\"][query_id].append((score[\"triangle\"], ext_doc_id))\n",
    "            data[\"cosine\"][query_id].append((score[\"cosine\"], ext_doc_id))\n",
    "            data[\"circle\"][query_id].append((score[\"circle\"], ext_doc_id))\n",
    "            data[\"passage\"][query_id].append((score[\"passage\"], ext_doc_id))\n",
    "            \n",
    "    # Append rest of rankings taken from tf-idf\n",
    "    for query_id in tokenized_queries.keys():\n",
    "        for _, row in rest.loc[rest['qid'] == int(query_id)][[\"docid\", \"similarity\"]].iterrows():\n",
    "            ext_doc_id = row[\"docid\"]\n",
    "            score = row[\"similarity\"]\n",
    "            data[\"gaussian\"][query_id].append((score, ext_doc_id))\n",
    "            data[\"triangle\"][query_id].append((score, ext_doc_id))\n",
    "            data[\"cosine\"][query_id].append((score, ext_doc_id))\n",
    "            data[\"circle\"][query_id].append((score, ext_doc_id))\n",
    "            data[\"passage\"][query_id].append((score, ext_doc_id))\n",
    "    \n",
    "    # Transform to tuples\n",
    "    for kernel in data.keys():\n",
    "        for query in data[kernel].keys():\n",
    "            data[kernel][query_id] = tuple(data[kernel][query_id])\n",
    "    \n",
    "        # Save to file\n",
    "        model_name = \"plm_\" + kernel + \"_\" + str(mu)\n",
    "        run_out_path = model_name + \".run\"\n",
    "        with open(run_out_path, 'w') as f_out:\n",
    "            write_run(\n",
    "                model_name=model_name,\n",
    "                data=data[kernel],\n",
    "                out_f=f_out,\n",
    "                max_objects_per_query=1000)\n",
    "    \n",
    "    print('Retrieved in {0} seconds'.format(time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF results upon the validation (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3672\n",
      "MAP@1000:\t0.2403\n",
      "Precision@5:\t0.3667\n",
      "Recall@1000:\t0.6712\n"
     ]
    }
   ],
   "source": [
    "TFIDF_results_val = evaluate(validation_set, \"tfidf\")\n",
    "print(\"TF-IDF has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(TFIDF_results_val['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(TFIDF_results_val['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(TFIDF_results_val['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(TFIDF_results_val['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLM ranking for μ=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM with Gaussian kernel and μ=500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3713\n",
      "MAP@1000:\t0.2424\n",
      "Precision@5:\t0.3800\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Triangle kernel and μ=500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3838\n",
      "MAP@1000:\t0.2457\n",
      "Precision@5:\t0.3667\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Cosine kernel and μ=500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3734\n",
      "MAP@1000:\t0.2445\n",
      "Precision@5:\t0.3600\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Circle kernel and μ=500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3770\n",
      "MAP@1000:\t0.2425\n",
      "Precision@5:\t0.3733\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Passage kernel and μ=500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3773\n",
      "MAP@1000:\t0.2419\n",
      "Precision@5:\t0.4067\n",
      "Recall@1000:\t0.6712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_PLM(tfidf_docs100, rest_ranking100, 500)\n",
    "PLM_Gaussian_500_results = evaluate(validation_set, \"plm_gaussian_500\")\n",
    "print(\"PLM with Gaussian kernel and μ=500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Gaussian_500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Gaussian_500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Gaussian_500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Gaussian_500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Triangle_500_results = evaluate(validation_set, \"plm_triangle_500\")\n",
    "print(\"PLM with Triangle kernel and μ=500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Triangle_500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Triangle_500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Triangle_500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Triangle_500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Cosine_500_results = evaluate(validation_set, \"plm_cosine_500\")\n",
    "print(\"PLM with Cosine kernel and μ=500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Cosine_500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Cosine_500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Cosine_500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Cosine_500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Circle_500_results = evaluate(validation_set, \"plm_circle_500\")\n",
    "print(\"PLM with Circle kernel and μ=500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Circle_500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Circle_500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Circle_500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Circle_500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Passage_500_results = evaluate(validation_set, \"plm_passage_500\")\n",
    "print(\"PLM with Passage kernel and μ=500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Passage_500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Passage_500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Passage_500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Passage_500_results['recall_1000']['all']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLM ranking for μ=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM with Gaussian kernel and μ=1000 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3726\n",
      "MAP@1000:\t0.2430\n",
      "Precision@5:\t0.3933\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Triangle kernel and μ=1000 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3877\n",
      "MAP@1000:\t0.2457\n",
      "Precision@5:\t0.3800\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Cosine kernel and μ=1000 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3754\n",
      "MAP@1000:\t0.2440\n",
      "Precision@5:\t0.3733\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Circle kernel and μ=1000 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3739\n",
      "MAP@1000:\t0.2426\n",
      "Precision@5:\t0.3667\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Passage kernel and μ=1000 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3725\n",
      "MAP@1000:\t0.2418\n",
      "Precision@5:\t0.3933\n",
      "Recall@1000:\t0.6712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_PLM(tfidf_docs100, rest_ranking100, 1000)\n",
    "PLM_Gaussian_1000_results = evaluate(validation_set, \"plm_gaussian_1000\")\n",
    "print(\"PLM with Gaussian kernel and μ=1000 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Gaussian_1000_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Gaussian_1000_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Gaussian_1000_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Gaussian_1000_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Triangle_1000_results = evaluate(validation_set, \"plm_triangle_1000\")\n",
    "print(\"PLM with Triangle kernel and μ=1000 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Triangle_1000_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Triangle_1000_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Triangle_1000_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Triangle_1000_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Cosine_1000_results = evaluate(validation_set, \"plm_cosine_1000\")\n",
    "print(\"PLM with Cosine kernel and μ=1000 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Cosine_1000_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Cosine_1000_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Cosine_1000_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Cosine_1000_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Circle_1000_results = evaluate(validation_set, \"plm_circle_1000\")\n",
    "print(\"PLM with Circle kernel and μ=1000 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Circle_1000_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Circle_1000_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Circle_1000_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Circle_1000_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Passage_1000_results = evaluate(validation_set, \"plm_passage_1000\")\n",
    "print(\"PLM with Passage kernel and μ=1000 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Passage_1000_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Passage_1000_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Passage_1000_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Passage_1000_results['recall_1000']['all']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLM ranking for μ=1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM with Gaussian kernel and μ=1500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3759\n",
      "MAP@1000:\t0.2434\n",
      "Precision@5:\t0.4000\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Triangle kernel and μ=1500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3817\n",
      "MAP@1000:\t0.2451\n",
      "Precision@5:\t0.3800\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Cosine kernel and μ=1500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3690\n",
      "MAP@1000:\t0.2439\n",
      "Precision@5:\t0.3733\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Circle kernel and μ=1500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3756\n",
      "MAP@1000:\t0.2420\n",
      "Precision@5:\t0.3800\n",
      "Recall@1000:\t0.6712\n",
      "\n",
      "PLM with Passage kernel and μ=1500 has the following scores upon the validation set:\n",
      "nDCG@10:\t0.3703\n",
      "MAP@1000:\t0.2414\n",
      "Precision@5:\t0.4000\n",
      "Recall@1000:\t0.6712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_PLM(tfidf_docs100, rest_ranking100, 1500)\n",
    "PLM_Gaussian_1500_results = evaluate(validation_set, \"plm_gaussian_1500\")\n",
    "print(\"PLM with Gaussian kernel and μ=1500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Gaussian_1500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Gaussian_1500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Gaussian_1500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Gaussian_1500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Triangle_1500_results = evaluate(validation_set, \"plm_triangle_1500\")\n",
    "print(\"PLM with Triangle kernel and μ=1500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Triangle_1500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Triangle_1500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Triangle_1500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Triangle_1500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Cosine_1500_results = evaluate(validation_set, \"plm_cosine_1500\")\n",
    "print(\"PLM with Cosine kernel and μ=1500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Cosine_1500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Cosine_1500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Cosine_1500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Cosine_1500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Circle_1500_results = evaluate(validation_set, \"plm_circle_1500\")\n",
    "print(\"PLM with Circle kernel and μ=1500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Circle_1500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Circle_1500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Circle_1500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Circle_1500_results['recall_1000']['all']))\n",
    "print()\n",
    "\n",
    "PLM_Passage_1500_results = evaluate(validation_set, \"plm_passage_1500\")\n",
    "print(\"PLM with Passage kernel and μ=1500 has the following scores upon the validation set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_Passage_1500_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_Passage_1500_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_Passage_1500_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_Passage_1500_results['recall_1000']['all']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the nDCG@10 scores upon the validation set for each kernel and for each chosen value of the Dirichlet hyperparameter μ.\n",
    "\n",
    "\n",
    "|              |  Gaussian | Triangle   | Cosine | Circle | Passage |\n",
    "|:------------:|:---------:|:----------:|:------:|:------:|:-------:|\n",
    "| **μ = 500**  | 0.3713    | **0.3838** | 0.3734 | 0.3770 | 0.3773  |\n",
    "| **μ = 1000** | 0.3726    | **0.3877** | 0.3754 | 0.3739 | 0.3725  |\n",
    "| **μ = 1500** | 0.3759    | **0.3817** | 0.3690 | 0.3756 | 0.3703  |\n",
    "\n",
    "We can see that Triangle kernel outperforms all the other kernel. The chosen value is μ=1000 where the triangle kernel achieves the highest score upon the reranking of the 100 documents. Note that the score of TF-IDF upon the validation set is 0.3672.\n",
    "\n",
    "#### PLM results upon the test set with Triangle Kernel and for μ=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM with Triangle kernel and μ=1000 has the following scores upon the test set:\n",
      "nDCG@10:\t0.3729\n",
      "MAP@1000:\t0.2037\n",
      "Precision@5:\t0.3867\n",
      "Recall@1000:\t0.6510\n"
     ]
    }
   ],
   "source": [
    "PLM_results = evaluate(test_set, \"plm_triangle_1000\")\n",
    "print(\"PLM with Triangle kernel and μ=1000 has the following scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(PLM_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(PLM_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(PLM_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(PLM_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for plotting the Language Models has been moved to the Appendix and has been commented out. This was decided because it is time-consuming to create all the plots. \n",
    "If one wants to reproduce the results, they can uncomment and run the code in the Appendix section.\n",
    "\n",
    "**Note:** Code for PLM score plotting can be found in this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(scores, parameters, param_name):\n",
    "    plt.plot(parameters, scores)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('nDCG@10 score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek Mercer scores upon validation set for different λ values\n",
    "\n",
    "![Jelinek-Mercer scores](JM.png)\n",
    "\n",
    "#### Dirichlet Prior smoothing scores upon validation set for different μ values\n",
    "![Dirichlet Prior Smoothing scores](DP.png)\n",
    "\n",
    "#### Absolute Discounting scores upon validation set for different δ values\n",
    "![Absolute Discounting scores](AD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f9079add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_plm_results():\n",
    "    scores = [\n",
    "        [PLM_Gaussian_500_results['ndcg_cut_10']['all'], PLM_Gaussian_1000_results['ndcg_cut_10']['all'], PLM_Gaussian_1500_results['ndcg_cut_10']['all']],\n",
    "        [PLM_Triangle_500_results['ndcg_cut_10']['all'], PLM_Triangle_1000_results['ndcg_cut_10']['all'], PLM_Triangle_1500_results['ndcg_cut_10']['all']],\n",
    "        [PLM_Cosine_500_results['ndcg_cut_10']['all'], PLM_Cosine_1000_results['ndcg_cut_10']['all'], PLM_Cosine_1500_results['ndcg_cut_10']['all']],\n",
    "        [PLM_Circle_500_results['ndcg_cut_10']['all'], PLM_Circle_1000_results['ndcg_cut_10']['all'], PLM_Circle_1500_results['ndcg_cut_10']['all']],\n",
    "        [PLM_Passage_500_results['ndcg_cut_10']['all'], PLM_Passage_1000_results['ndcg_cut_10']['all'], PLM_Passage_1500_results['ndcg_cut_10']['all']]\n",
    "    ]\n",
    "    \n",
    "    parameters = [500,1000,1500]\n",
    "    \n",
    "    labels = [\"Gaussian\", \"Triangle\", \"Cosine\", \"Circle\", \"Passage\"]\n",
    "    \n",
    "    for ind, label in enumerate(labels):\n",
    "        num_scores = [float(s) for s in scores[ind]]\n",
    "        plt.plot(parameters, num_scores, label=label)\n",
    "    \n",
    "    # Add also TF-IDF\n",
    "    num_scores = 3 * [float(TFIDF_results_val['ndcg_cut_10']['all'])]\n",
    "    plt.plot(parameters, num_scores, label=\"TF-IDF\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"mu\")\n",
    "    plt.ylabel('nDCG@10 score')\n",
    "    plt.show()\n",
    "plot_plm_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Positional Language Model scores upon validation set for different μ values and kernels\n",
    "![PLM scores](PLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance of the results with the two-tailed paired Student t-test\n",
    "\n",
    "We apply statistical significance tests of the results using two-tailed paired Student t-tests.\n",
    "\n",
    "* Two-tailed paired Student t-test is applied upon two populations with the following hypotheses:\n",
    "    1. Null hypothesis $H_0$: the means of the two populations are equal.\n",
    "    2. Alternative hypothesis $H_a$: the means of the two populations are not equal.\n",
    "\n",
    "* For each possible pair of retrieval methods, we take their results which consist of 4 evaluation metrics.\n",
    "\n",
    "* Four evaluation metrics means that four experiments were conducted, therefore we compare each metric upon the pair results. Having 4 evaluation metrics and 5 different retrieval methods, we conculde that we have to perform (5!)*4 = 60 statistical significance tests.\n",
    "\n",
    "* In order to tackle the multiple comparisons problem between the 4 conducted experiments, we apply the Šidák correction which states that in each comparison we use:\n",
    "$$\\alpha_{\\text{{per comparison}}} = 1-(1-\\alpha)^{1/m}$$\n",
    "\n",
    ",where m=4 (the number of experiments) and α=0.05\n",
    "\n",
    "The code below creates a table presenting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistical significance with two-tailed paired Student t-test using Šidák correction\n",
      "Null hypothesis: equal averages.\n",
      "Alternative hypothesis: inequal averages.\n",
      "Conducted Experiments: 4\n",
      "alpha value: 0.012741455098566168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Model 1</th>\n",
       "      <th>Model 2</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.54616811868</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.739269264159</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_5</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.284916168242</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.840091387089</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>0.000239062778789</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>0.000116294645219</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P_5</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>0.000119902648562</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>0.000634426417932</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.588497434799</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.401969816411</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P_5</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.563207496249</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.0080888088633</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.0442159085371</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.00812534944018</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P_5</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.0584649779319</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>8.44627824543e-06</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00382304531482</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00747197307557</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P_5</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.0305789491145</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>1</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>4.49630610593e-07</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>3.00440427347e-12</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P_5</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>6.37735751608e-05</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>1.14028868565e-05</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.990796958749</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.0613812807381</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P_5</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.629614955317</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.000179501198694</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.0129334025037</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>3.59259521498e-07</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P_5</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.199809198008</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>2.59925768793e-08</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>BM25</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00750421123995</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00370570103378</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>P_5</td>\n",
       "      <td>BM25</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.134603659475</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>BM25</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.840091387089</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>2.53572920794e-05</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>2.0240199645e-06</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.000291949771851</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>0.0526443837015</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.000918257062119</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.000421706512776</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.000256102269342</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.406973403099</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.100906099407</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00575631636349</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.0413997278186</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Jelinek-Mercer</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.000634426417932</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.0507494607671</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.0671093351323</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.178719370632</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>0.711556493065</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.00953389204242</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.200976628039</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.0863760015091</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Dirichlet Prior Smoothing</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.0080888088633</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.34939233637</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.864413164979</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>P_5</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>0.526791412864</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>Absolute Discounting</td>\n",
       "      <td>PLM with Triangle Kernel</td>\n",
       "      <td>8.44627824543e-06</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric                    Model 1                    Model 2  \\\n",
       "1    ndcg_cut_10                     TF-IDF                       BM25   \n",
       "2   map_cut_1000                     TF-IDF                       BM25   \n",
       "3            P_5                     TF-IDF                       BM25   \n",
       "4    recall_1000                     TF-IDF                       BM25   \n",
       "5    ndcg_cut_10                     TF-IDF             Jelinek-Mercer   \n",
       "6   map_cut_1000                     TF-IDF             Jelinek-Mercer   \n",
       "7            P_5                     TF-IDF             Jelinek-Mercer   \n",
       "8    recall_1000                     TF-IDF             Jelinek-Mercer   \n",
       "9    ndcg_cut_10                     TF-IDF  Dirichlet Prior Smoothing   \n",
       "10  map_cut_1000                     TF-IDF  Dirichlet Prior Smoothing   \n",
       "11           P_5                     TF-IDF  Dirichlet Prior Smoothing   \n",
       "12   recall_1000                     TF-IDF  Dirichlet Prior Smoothing   \n",
       "13   ndcg_cut_10                     TF-IDF       Absolute Discounting   \n",
       "14  map_cut_1000                     TF-IDF       Absolute Discounting   \n",
       "15           P_5                     TF-IDF       Absolute Discounting   \n",
       "16   recall_1000                     TF-IDF       Absolute Discounting   \n",
       "17   ndcg_cut_10                     TF-IDF   PLM with Triangle Kernel   \n",
       "18  map_cut_1000                     TF-IDF   PLM with Triangle Kernel   \n",
       "19           P_5                     TF-IDF   PLM with Triangle Kernel   \n",
       "20   recall_1000                     TF-IDF   PLM with Triangle Kernel   \n",
       "21   ndcg_cut_10                       BM25             Jelinek-Mercer   \n",
       "22  map_cut_1000                       BM25             Jelinek-Mercer   \n",
       "23           P_5                       BM25             Jelinek-Mercer   \n",
       "24   recall_1000                       BM25             Jelinek-Mercer   \n",
       "25   ndcg_cut_10                       BM25  Dirichlet Prior Smoothing   \n",
       "26  map_cut_1000                       BM25  Dirichlet Prior Smoothing   \n",
       "27           P_5                       BM25  Dirichlet Prior Smoothing   \n",
       "28   recall_1000                       BM25  Dirichlet Prior Smoothing   \n",
       "29   ndcg_cut_10                       BM25       Absolute Discounting   \n",
       "30  map_cut_1000                       BM25       Absolute Discounting   \n",
       "31           P_5                       BM25       Absolute Discounting   \n",
       "32   recall_1000                       BM25       Absolute Discounting   \n",
       "33   ndcg_cut_10                       BM25   PLM with Triangle Kernel   \n",
       "34  map_cut_1000                       BM25   PLM with Triangle Kernel   \n",
       "35           P_5                       BM25   PLM with Triangle Kernel   \n",
       "36   recall_1000                       BM25   PLM with Triangle Kernel   \n",
       "37   ndcg_cut_10             Jelinek-Mercer  Dirichlet Prior Smoothing   \n",
       "38  map_cut_1000             Jelinek-Mercer  Dirichlet Prior Smoothing   \n",
       "39           P_5             Jelinek-Mercer  Dirichlet Prior Smoothing   \n",
       "40   recall_1000             Jelinek-Mercer  Dirichlet Prior Smoothing   \n",
       "41   ndcg_cut_10             Jelinek-Mercer       Absolute Discounting   \n",
       "42  map_cut_1000             Jelinek-Mercer       Absolute Discounting   \n",
       "43           P_5             Jelinek-Mercer       Absolute Discounting   \n",
       "44   recall_1000             Jelinek-Mercer       Absolute Discounting   \n",
       "45   ndcg_cut_10             Jelinek-Mercer   PLM with Triangle Kernel   \n",
       "46  map_cut_1000             Jelinek-Mercer   PLM with Triangle Kernel   \n",
       "47           P_5             Jelinek-Mercer   PLM with Triangle Kernel   \n",
       "48   recall_1000             Jelinek-Mercer   PLM with Triangle Kernel   \n",
       "49   ndcg_cut_10  Dirichlet Prior Smoothing       Absolute Discounting   \n",
       "50  map_cut_1000  Dirichlet Prior Smoothing       Absolute Discounting   \n",
       "51           P_5  Dirichlet Prior Smoothing       Absolute Discounting   \n",
       "52   recall_1000  Dirichlet Prior Smoothing       Absolute Discounting   \n",
       "53   ndcg_cut_10  Dirichlet Prior Smoothing   PLM with Triangle Kernel   \n",
       "54  map_cut_1000  Dirichlet Prior Smoothing   PLM with Triangle Kernel   \n",
       "55           P_5  Dirichlet Prior Smoothing   PLM with Triangle Kernel   \n",
       "56   recall_1000  Dirichlet Prior Smoothing   PLM with Triangle Kernel   \n",
       "57   ndcg_cut_10       Absolute Discounting   PLM with Triangle Kernel   \n",
       "58  map_cut_1000       Absolute Discounting   PLM with Triangle Kernel   \n",
       "59           P_5       Absolute Discounting   PLM with Triangle Kernel   \n",
       "60   recall_1000       Absolute Discounting   PLM with Triangle Kernel   \n",
       "\n",
       "              p-value       Outcome  \n",
       "1       0.54616811868  not rejected  \n",
       "2      0.739269264159  not rejected  \n",
       "3      0.284916168242  not rejected  \n",
       "4      0.840091387089  not rejected  \n",
       "5   0.000239062778789      rejected  \n",
       "6   0.000116294645219      rejected  \n",
       "7   0.000119902648562      rejected  \n",
       "8   0.000634426417932      rejected  \n",
       "9      0.588497434799  not rejected  \n",
       "10     0.401969816411  not rejected  \n",
       "11     0.563207496249  not rejected  \n",
       "12    0.0080888088633      rejected  \n",
       "13    0.0442159085371  not rejected  \n",
       "14   0.00812534944018      rejected  \n",
       "15    0.0584649779319  not rejected  \n",
       "16  8.44627824543e-06      rejected  \n",
       "17   0.00382304531482      rejected  \n",
       "18   0.00747197307557      rejected  \n",
       "19    0.0305789491145  not rejected  \n",
       "20                  1  not rejected  \n",
       "21  4.49630610593e-07      rejected  \n",
       "22  3.00440427347e-12      rejected  \n",
       "23  6.37735751608e-05      rejected  \n",
       "24  1.14028868565e-05      rejected  \n",
       "25     0.990796958749  not rejected  \n",
       "26    0.0613812807381  not rejected  \n",
       "27     0.629614955317  not rejected  \n",
       "28  0.000179501198694      rejected  \n",
       "29    0.0129334025037  not rejected  \n",
       "30  3.59259521498e-07      rejected  \n",
       "31     0.199809198008  not rejected  \n",
       "32  2.59925768793e-08      rejected  \n",
       "33   0.00750421123995      rejected  \n",
       "34   0.00370570103378      rejected  \n",
       "35     0.134603659475  not rejected  \n",
       "36     0.840091387089  not rejected  \n",
       "37  2.53572920794e-05      rejected  \n",
       "38   2.0240199645e-06      rejected  \n",
       "39  0.000291949771851      rejected  \n",
       "40    0.0526443837015  not rejected  \n",
       "41  0.000918257062119      rejected  \n",
       "42  0.000421706512776      rejected  \n",
       "43  0.000256102269342      rejected  \n",
       "44     0.406973403099  not rejected  \n",
       "45     0.100906099407  not rejected  \n",
       "46   0.00575631636349      rejected  \n",
       "47    0.0413997278186  not rejected  \n",
       "48  0.000634426417932      rejected  \n",
       "49    0.0507494607671  not rejected  \n",
       "50    0.0671093351323  not rejected  \n",
       "51     0.178719370632  not rejected  \n",
       "52     0.711556493065  not rejected  \n",
       "53   0.00953389204242      rejected  \n",
       "54     0.200976628039  not rejected  \n",
       "55    0.0863760015091  not rejected  \n",
       "56    0.0080888088633      rejected  \n",
       "57      0.34939233637  not rejected  \n",
       "58     0.864413164979  not rejected  \n",
       "59     0.526791412864  not rejected  \n",
       "60  8.44627824543e-06      rejected  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_models = {\n",
    "    \"tfidf\" : \"TF-IDF\", \"bm25\" : \"BM25\", \"JelinekMercer01\" : \"Jelinek-Mercer\", \n",
    "    \"DirichletPrior500\" : \"Dirichlet Prior Smoothing\", \"AbsoluteDiscounting09\" : \"Absolute Discounting\", \n",
    "    \"plm_triangle_1000\" : \"PLM with Triangle Kernel\"\n",
    "}\n",
    "\n",
    "def compute_statistical_significance(run_models):\n",
    "    \"\"\"Applies two-tailed paired Student t-test upon each pair of retrieval methods for each score.\n",
    "    \n",
    "    Keyword parameters:\n",
    "    run_models -- a dictionary with keys the run model filenames and values the corresponding retrieval method name.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Result list\n",
    "    results = []\n",
    "    \n",
    "    # Calibrate the level of α according to Šidák correction for 4 experiments. \n",
    "    alpha = 1 - (1-0.05)**(1/4)\n",
    "    \n",
    "    print(\"Computing statistical significance with two-tailed paired Student t-test using Šidák correction\")\n",
    "    print(\"Null hypothesis: equal averages.\")\n",
    "    print(\"Alternative hypothesis: inequal averages.\")\n",
    "    print(\"Conducted Experiments: 4\")\n",
    "    print(\"alpha value:\", alpha)\n",
    "    \n",
    "    # For each run model\n",
    "    for i, run in enumerate(run_models):\n",
    "        models_to_compare = list(run_models.keys())[(i+1):]\n",
    "        if models_to_compare:\n",
    "            # Take any other model to compare with\n",
    "            for comp_run in models_to_compare:\n",
    "                # Get the scores of each run file\n",
    "                a = evaluate(test_set, run)\n",
    "                b = evaluate(test_set, comp_run)\n",
    "                # For each score metric\n",
    "                for metric in [\"ndcg_cut_10\", \"map_cut_1000\", \"P_5\", \"recall_1000\"]:\n",
    "                    x = [float(a[metric][key]) for key in a[metric].keys() if key != \"all\"]\n",
    "                    y = [float(b[metric][key]) for key in b[metric].keys() if key != \"all\"]\n",
    "                    _, p = ttest_rel(x,y)\n",
    "                    if p > alpha or math.isnan(p):\n",
    "                        results.append([metric, run_models[run], run_models[comp_run], str(1) if math.isnan(p) else str(p), \"not rejected\"])\n",
    "                    elif p <= alpha:\n",
    "                        results.append([metric, run_models[run], run_models[comp_run], str(p), \"rejected\"])\n",
    "    table = pd.DataFrame(results, columns=[\"Metric\", \"Model 1\", \"Model 2\", \"p-value\", \"Outcome\"])\n",
    "    table.index = np.arange(1, len(table) + 1)\n",
    "    display(table)\n",
    "    return table\n",
    "\n",
    "table = compute_statistical_significance(run_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "import os\n",
    "from gensim.models import lsimodel \n",
    "from gensim.models import tfidfmodel\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from scipy import spatial\n",
    "import heapq\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING CORPUS AND DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"outfile\"):\n",
    "    #tokenize each document and save it to an array \n",
    "    all_docs=[]\n",
    "    for doc_id in range(index.document_base(),index.maximum_document()):\n",
    "        example_document,doc = index.document(doc_id)\n",
    "        all_docs.append([id2token[word_id] for word_id in doc if word_id > 0])\n",
    "\n",
    "    with open('outfile', 'wb') as fp:\n",
    "        pickle.dump(all_docs, fp)\n",
    "else:\n",
    "    with open ('outfile', 'rb') as fp:\n",
    "        all_docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if not os.path.isfile(\"corpus_ls\"):\n",
    "\n",
    "    #Create & Save  Dictionary\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary(documents=all_docs)\n",
    "    dictionary.save(\"dictionary_ls\")\n",
    "    #Create & Save Corpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in  all_docs]\n",
    "    corpora.MmCorpus.serialize(\"corpus_ls\", corpus)\n",
    "    #corpus.save(\"corpus_ls\")\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(\"dictionary_ls\")\n",
    "    corpus = corpora.MmCorpus(\"corpus_ls\")\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"lsi_model\"):\n",
    "    # first, transform word counts to tf-idf weights\n",
    "    tfidf_model = tfidfmodel.TfidfModel(corpus, id2word = dictionary, normalize = True)\n",
    "    \n",
    "    # then find the transformation from tf-idf to latent space\n",
    "    lsi_model = lsimodel.LsiModel(tfidf_model[corpus], id2word = dictionary, num_topics = 64)\n",
    "    \n",
    "    # Save the models to disk\n",
    "    tfidf_model.save(\"tfidf_model\")\n",
    "    lsi_model.save(\"lsi_model\")\n",
    "else:\n",
    "    tfidf_model = gensim.models.TfidfModel.load(\"tfidf_model\")\n",
    "    lsi_model = gensim.models.LsiModel.load(\"lsi_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel  \n",
    "\n",
    "if not os.path.isfile(\"lda_model\"):\n",
    "    \n",
    "    lda_model = ldamodel.LdaModel(corpus, id2word = dictionary, num_topics = 64)\n",
    "    lda_model.save(\"lda_model\")\n",
    "else:\n",
    "    lda_model = gensim.models.LdaModel.load(\"lda_model\") \n",
    "\n",
    "num_topics = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"tfidf_docs1000\"):\n",
    "    tfidf_docs1000, rest_ranking1000 = get_tfidf_rankings(1000)\n",
    "\n",
    "    with open('tfidf_docs1000', 'wb') as fp:\n",
    "        pickle.dump(tfidf_docs1000, fp)\n",
    "else:\n",
    "    with open ('tfidf_docs1000', 'rb') as fp:\n",
    "        tfidf_docs1000 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsi_score(tfidf_docs1000):\n",
    "    \"\"\"\n",
    "    LSI\n",
    "    Transform each query and each document with LSI model\n",
    "    Cosine similarity to the two vec QueryLsi and DocLSI\n",
    "    \n",
    "    \"\"\"\n",
    "    filename='./ap_88_89/topics_title'\n",
    "\n",
    "    with open(filename, 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "        \n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    for doc_id in tfidf_docs1000.keys():\n",
    "        for query_id in tfidf_docs1000[doc_id]:\n",
    "            \n",
    "            #constructing the query representation######\n",
    "            query = queries[query_id]\n",
    "            # Tokenize the query\n",
    "            tokenized_query = index.tokenize(query)\n",
    "            # Get token ids for tokens in the query\n",
    "            token_ids = [token2id.get(token, 0) for token in tokenized_query]\n",
    "            # Remove stopwords\n",
    "            token_ids = [id2token[t] for t in token_ids if t > 0]\n",
    "\n",
    "            query_bow = dictionary.doc2bow(token_ids)\n",
    "            query_lsi = lsi_model[tfidf_model[query_bow]]\n",
    "            query_lsi = [x[1] for x in query_lsi]\n",
    "            ##############################\n",
    "\n",
    "            #construction the doc representation\n",
    "            example_document,doc = index.document(doc_id)\n",
    "            doc = [id2token[word_id] for word_id in doc if word_id > 0]                        \n",
    "            doc_bow = dictionary.doc2bow(doc)\n",
    "            doc_lsi = lsi_model[tfidf_model[doc_bow]]\n",
    "            doc_lsi = [x[1] for x in doc_lsi]\n",
    "            \n",
    "            #score similarity \n",
    "            scores[query_id].append((doc_id , 1-spatial.distance.cosine(query_lsi,doc_lsi)))\n",
    "            \n",
    "            #sort the top1000 scores for each query\n",
    "            for query_id in scores:\n",
    "                scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(1))\n",
    "\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"LSI_tfidf_scores\"):\n",
    "    scores_lsi = lsi_score(tfidf_docs1000)\n",
    "\n",
    "    with open('LSI_tfidf_scores', 'wb') as fp:\n",
    "        pickle.dump(scores_lsi, fp)\n",
    "else:\n",
    "    with open ('LSI_tfidf_scores', 'rb') as fp:\n",
    "        scores_lsi = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('LSI_tfidf_scores.run', 'w') as the_file:\n",
    "    for query_id in scores_lsi:\n",
    "        for i in range(len(scores_lsi[query_id])):\n",
    "            the_file.write(query_id+\" Q0 \"+index.document(scores_lsi[query_id][i][0])[0]+\" \"+str(i)+\" \"+str(scores_lsi[query_id][i][1])+\" \"+\"LSI_tfidf_scores\\n\")\n",
    "    the_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSI_results = evaluate(test_set, \"LSI_tfidf_scores\")\n",
    "print(\"LSI has the f|ollowing scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(LSI_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(LSI_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(LSI_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(LSI_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_score(tfidf_docs1000):\n",
    "\n",
    "    filename='./ap_88_89/topics_title'\n",
    "\n",
    "    with open(filename, 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "\n",
    "    for doc_id in tfidf_docs1000.keys():\n",
    "        for query_id in tfidf_docs1000[doc_id]:\n",
    "            \n",
    "            #constructing the query repr######\n",
    "            query = queries[query_id]\n",
    "            # Tokenize the query\n",
    "            tokenized_query = index.tokenize(query)\n",
    "            # Get token ids for tokens in the query\n",
    "            token_ids = [token2id.get(token, 0) for token in tokenized_query]\n",
    "            # Remove stopwords\n",
    "            #token_ids = [t for t in token_ids if t > 0]\n",
    "            token_ids = [id2token[t] for t in  token_ids if t > 0]\n",
    "            query_bow = dictionary.doc2bow(token_ids)\n",
    "            query_lda = lda_model[query_bow]\n",
    "            \n",
    "            flag = False\n",
    "            for i in range(0,64):\n",
    "                for item in query_lda:\n",
    "                    if item[0] == i:\n",
    "                        flag = True\n",
    "                if(flag == False):\n",
    "                    query_lda.append((i,0))\n",
    "                flag = False\n",
    "            \n",
    "            query_lda = sorted(query_lda, key=lambda x: x[0])\n",
    "            query_lda = [x[1] for x in query_lda]\n",
    "            ##############################\n",
    "\n",
    "            \n",
    "            example_document,doc = index.document(doc_id)\n",
    "            doc = [id2token[word_id] for word_id in doc if word_id > 0]            \n",
    "            #doc_terms = Counter(token_ids)            \n",
    "            doc_bow = dictionary.doc2bow(doc)\n",
    "            doc_lda = lda_model[doc_bow]\n",
    "            flag2 = False\n",
    "            #make 64 as num of topics parmtr\n",
    "            for i in range(0,64):\n",
    "                for item in doc_lda:\n",
    "                    if item[0] == i:\n",
    "                        flag2 = True\n",
    "                if(flag2 == False):\n",
    "                    doc_lda.append((i,0))\n",
    "                flag2 = False\n",
    "            doc_lda = sorted(doc_lda, key=lambda x: x[0])\n",
    "            doc_lda = [x[1] for x in doc_lda]\n",
    "            \n",
    "            scores[query_id].append((doc_id , 1-spatial.distance.cosine(query_lda,doc_lda)))\n",
    "\n",
    "\n",
    "    for query_id in scores:\n",
    "        scores[query_id] = heapq.nlargest(1000, scores[query_id], itemgetter(1))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"LDA_tfidf_scores\"):\n",
    "    scores_lda = lda_score(tfidf_docs1000)\n",
    "\n",
    "    with open('LDA_tfidf_scores', 'wb') as fp:\n",
    "        pickle.dump(scores_lda, fp)\n",
    "else:\n",
    "    with open ('LDA_tfidf_scores', 'rb') as fp:\n",
    "        scores_lda = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the run scores file\n",
    "\n",
    "with open('LDA_tfidf_scores.run', 'w') as the_file:\n",
    "    for query_id in scores_lda:\n",
    "        for i in range(len(scores_lda[query_id])):\n",
    "            the_file.write(query_id+\" Q0 \"+index.document(scores_lda[query_id][i][0])[0]+\" \"+str(i)+\" \"+str(scores_lda[query_id][i][1])+\" \"+\"LSA_tfidf_scores\\n\")\n",
    "    the_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_results = evaluate(test_set, \"LDA_tfidf_scores\")\n",
    "print(\"LDA has the f|ollowing scores upon the test set:\")\n",
    "print(\"nDCG@10:\\t{0}\".format(LDA_results['ndcg_cut_10']['all']))\n",
    "print(\"MAP@1000:\\t{0}\".format(LDA_results['map_cut_1000']['all']))\n",
    "print(\"Precision@5:\\t{0}\".format(LDA_results['P_5']['all']))\n",
    "print(\"Recall@1000:\\t{0}\".format(LDA_results['recall_1000']['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_models = {\n",
    "    \"tfidf\" : \"TF-IDF\", \"LSI_tfidf_scores\" : \"LSI on best-tfidf-pairs\", \n",
    "    \"LDA_tfidf_scores\" : \"LDA on best-tfidf-pairs\"\n",
    "}\n",
    "\n",
    "table2 = compute_statistical_significance(run_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment.\n",
    "\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/pp1291-Nalisnick.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyndri.compat\n",
    "\n",
    "## Train the model\n",
    "model = gensim.models.Word2Vec(\n",
    "    size=500,  # Embedding size\n",
    "    iter=10,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "# Build vocab.\n",
    "model.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"10Epochs_500\")\n",
    "\n",
    "print('Trained model = ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def embedAllDocuments():\n",
    "    doc2embed = {}\n",
    "    model = Word2Vec.load(\"10Epochs_500\")\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        if document_lengths[int_doc_id] > 0:\n",
    "            ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "            D = np.zeros(500)\n",
    "            for d in doc_token_ids:\n",
    "                if d != 0:\n",
    "                    try:\n",
    "                        dj = model.wv[dictionary[d]] / np.linalg.norm(model.wv[dictionary[d]])\n",
    "                        D += dj\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "            doc2embed[int_doc_id] = (1/document_lengths[int_doc_id]) * D\n",
    "            if int_doc_id % 10000 == 0:\n",
    "                print(\"Document \" + str(int_doc_id) + \" out of \" + str(index.maximum_document()))\n",
    "    return doc2embed\n",
    "\n",
    "doc2embed = embedAllDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('doc2embed20.pickle', 'wb') as handle:\n",
    "    pickle.dump(doc2embed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('doc2embed20.pickle', 'rb') as handle:\n",
    "    doc2embed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval2(model_name, score_fn):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for doc in range(index.document_base(), num_documents+1):\n",
    "        if doc % 10000 == 0: \n",
    "            print(str(doc) + \" out of \" + str(index.maximum_document()))\n",
    "        # Skip empty documents\n",
    "        if index.document_length(doc) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get document id\n",
    "        ext_doc_id = index.document(doc)[0]\n",
    "        \n",
    "        # Iterate over topic queries\n",
    "        for query_id in tokenized_queries.keys():\n",
    "            if query_id not in data.keys():\n",
    "                data[query_id] = []\n",
    "            # Iterate over terms of topic query and calculate score\n",
    "            score = score_fn(doc, query_id)\n",
    "            data[query_id].append(tuple(((score, ext_doc_id))))\n",
    "    # Save to file\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    \n",
    "    print('Retrieved in {0} seconds'.format(time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model = Word2Vec.load(\"10Epochs_500\")\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "def wordEmbedingForRanking(int_document_id, query_id):\n",
    "    desm = 0\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        try:\n",
    "            x = np.array([np.transpose(model.wv[dictionary[query_term_id]])])\n",
    "            y = doc2embed[int_document_id]\n",
    "            desm += (np.dot(x, y) / (np.linalg.norm(model.wv[dictionary[query_term_id]]) * np.linalg.norm(y)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    desm = (1/len(tokenized_queries[query_id])) * desm\n",
    "    return desm[0]\n",
    "run_retrieval2(\"wordEmbedingForRanking\", wordEmbedingForRanking)\n",
    "# tokenized_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_tfidf_rankings(n=100):\n",
    "    \n",
    "    # Get a mapping from document names (e.g. \"AP880318-0287) to their dictionary IDs.\n",
    "    doc2id = {}\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        doc2id[index.document(int_doc_id)[0]] = int_doc_id\n",
    "        \n",
    "    # Get tfidf rankings\n",
    "    names = [\"qid\", \"Q0\", \"docid\", \"rank\", \"similarity\", \"run\"]\n",
    "    data = pd.read_csv('tfidf.run', sep=' ', header=None, names=names)\n",
    "    \n",
    "    #data = data[[\"qid\", \"docid\"]]\n",
    "    last_data = pd.DataFrame(columns=[\"qid\", \"docid\"])\n",
    "    final_data = pd.DataFrame(columns=[\"qid\", \"docid\"])\n",
    "    for query in tokenized_queries.keys():\n",
    "        temp = data.loc[data['qid'] == int(query)]\n",
    "        \n",
    "        # Take rest of rankings\n",
    "        rest = data.loc[data['qid'] == int(query)].tail(1000-n)\n",
    "        \n",
    "        # Add large negative values to similarity\n",
    "        rest[\"similarity\"] = range(-9999 ,-9999-rest[\"similarity\"].shape[0], -1)\n",
    "        \n",
    "        # Append to last rankings table\n",
    "        last_data = last_data.append(rest)\n",
    "        \n",
    "        # Get the top n rankings\n",
    "        final_data = final_data.append(temp.head(n))\n",
    "    \n",
    "    # Create a dictionary for the top n rankings:\n",
    "    # results: doc_id -> list of queries which have this doc in their top n rankings\n",
    "    final_data = final_data[[\"qid\", \"docid\"]]\n",
    "    results = defaultdict(list)\n",
    "    for _, row in final_data.iterrows():\n",
    "        results[doc2id[row[\"docid\"]]].append(str(row[\"qid\"]))\n",
    "        \n",
    "    return results, last_data\n",
    "\n",
    "def get_test_set_top(n=5):\n",
    "    \n",
    "    # Get a mapping from document names (e.g. \"AP880318-0287) to their dictionary IDs.\n",
    "    doc2id = {}\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        doc2id[index.document(int_doc_id)[0]] = int_doc_id\n",
    "    results = {}\n",
    "    # Get tfidf rankings\n",
    "    names = [\"qid\", \"Q0\", \"docid\", \"rel\"]\n",
    "    data = pd.read_csv('ap_88_89/qrel_test', sep=' ', header=None, names=names)\n",
    "    unique_queries = data['qid'].unique()\n",
    "    for q in unique_queries:\n",
    "        docs = data.loc[data['qid'] == q][:5]['docid']\n",
    "        docids = [doc2id[doc] for doc in docs]\n",
    "        results[q] = docids\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def extract_features_and_labels(kfold_index, all_queries):\n",
    "    query_index_train = []\n",
    "    for rindex in kfold_index:\n",
    "        query_index_train.append(all_queries[rindex])\n",
    "    features = []\n",
    "    classes = []\n",
    "    all_query_id = []\n",
    "    documents = []\n",
    "    for document, queries in tfidf_docs1000.items():\n",
    "        for query in queries:\n",
    "            if query in query_index_train:\n",
    "                score_tf = 0\n",
    "                counter = 0\n",
    "                score_bm = 0\n",
    "                for query_term_id in tokenized_queries[query]:\n",
    "                    if document not in inverted_index[query_term_id]:\n",
    "                        document_term_freq = 0\n",
    "                    else:\n",
    "                        document_term_freq = inverted_index[query_term_id][document]\n",
    "                    score_tf += tfidf(document, query_term_id, document_term_freq)\n",
    "                    score_bm += BM25(document, query_term_id, document_term_freq)\n",
    "                    counter = counter +1\n",
    "                all_query_id.append(query)\n",
    "                documents.append(document)\n",
    "                features.append([score_tf, score_bm, counter])\n",
    "                classes.append(docquery2relivance[query][document])\n",
    "    return features, classes, all_query_id, documents\n",
    "\n",
    "def evaluate_ltr(test_index, predictions, queries, documents):\n",
    "    ranking = get_test_set_top(5)\n",
    "    for query_index in test_index:\n",
    "        query_id = all_queries[query_index]\n",
    "        indices = [index for index, value in enumerate(queries) if value == query_id]\n",
    "        query_pred = [queries[i] for i in indices]\n",
    "        doc_pred = [documents[i] for i in indices]\n",
    "        score_predictions = [predictions[i] for i in indices]\n",
    "\n",
    "        sorted_predictions, sorted_docs = zip(*sorted(zip(score_predictions, doc_pred)))\n",
    "        top5_docs_predicted = sorted_docs[-5:]\n",
    "        top5_docs_actual = ranking[int(query_id)]\n",
    "        TP = len(set(top5_docs_actual).intersection(top5_docs_predicted))\n",
    "        FP = 5-TP\n",
    "        precision = TP/(TP+FP)\n",
    "        allPrecision.append(precision)\n",
    "#         print(np.mean(allPrecision))\n",
    "    return allPrecision\n",
    "\n",
    "doc2id = {}\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    doc2id[index.document(int_doc_id)[0]] = int_doc_id\n",
    "\n",
    "docquery2relivance = defaultdict(lambda: defaultdict(int))\n",
    "with open(\"ap_88_89/qrel_test\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        try:\n",
    "            docquery2relivance[row[0]][doc2id[row[2]]] = int(row[3])\n",
    "        except KeyError:\n",
    "            print(row[0],row[2])\n",
    "            \n",
    "all_queries = list(docquery2relivance.keys())\n",
    "kf = KFold(n_splits=10)\n",
    "tfidf_docs1000, _ = get_tfidf_rankings(1000)\n",
    "precisionPerFold = []\n",
    "for train_index, test_index in kf.split(all_queries):\n",
    "    features, classes, _, _ = extract_features_and_labels(train_index,all_queries)\n",
    "    features_test, classes_test, queries_test, document_test = extract_features_and_labels(test_index,all_queries)\n",
    "    \n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(features, classes)\n",
    "    x = classifier.decision_function(features_test)\n",
    "    allPrecision = evaluate_ltr(test_index, x, queries_test, document_test)\n",
    "    meanPrecision = np.mean(allPrecision)\n",
    "    precisionPerFold.append(meanPrecision)\n",
    "print(np.mean(precisionPerFold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2id = {}\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    doc2id[index.document(int_doc_id)[0]] = int_doc_id\n",
    "print(doc2id['AP880218-0195'])\n",
    "print(doc2id['AP880229-0184'])\n",
    "print(doc2id['AP880301-0033'])\n",
    "print(doc2id['AP880302-0095'])\n",
    "print(doc2id['AP880312-0116'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task 5: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix\n",
    "Auxialiary code. Can be uncommented to reproduce results for:\n",
    "\n",
    "\n",
    "* Plotting the Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jelinek Mercer Plotting\n",
    "\n",
    "# JelinekMercer02 = partial(JelinekMercer, lambda_param = 0.2)\n",
    "# run_retrieval('JelinekMercer02', JelinekMercer02)\n",
    "# JelinekMercer03 = partial(JelinekMercer, lambda_param = 0.3)\n",
    "# run_retrieval('JelinekMercer03', JelinekMercer03)\n",
    "# JelinekMercer04 = partial(JelinekMercer, lambda_param = 0.4)\n",
    "# run_retrieval('JelinekMercer04', JelinekMercer04)\n",
    "# JelinekMercer06 = partial(JelinekMercer, lambda_param = 0.6)\n",
    "# run_retrieval('JelinekMercer06', JelinekMercer06)\n",
    "# JelinekMercer07 = partial(JelinekMercer, lambda_param = 0.7)\n",
    "# run_retrieval('JelinekMercer07', JelinekMercer07)\n",
    "# JelinekMercer08 = partial(JelinekMercer, lambda_param = 0.8)\n",
    "# run_retrieval('JelinekMercer08', JelinekMercer08)\n",
    "\n",
    "# JM_results_02 = evaluate(validation_set, \"JelinekMercer02\")\n",
    "# JM_results_03 = evaluate(validation_set, \"JelinekMercer03\")\n",
    "# JM_results_04 = evaluate(validation_set, \"JelinekMercer04\")\n",
    "# JM_results_06 = evaluate(validation_set, \"JelinekMercer06\")\n",
    "# JM_results_07 = evaluate(validation_set, \"JelinekMercer07\")\n",
    "# JM_results_08 = evaluate(validation_set, \"JelinekMercer08\")\n",
    "\n",
    "# scores = [JM_results_01['ndcg_cut_10']['all'],\n",
    "#          JM_results_02['ndcg_cut_10']['all'],\n",
    "#          JM_results_03['ndcg_cut_10']['all'],\n",
    "#          JM_results_04['ndcg_cut_10']['all'],\n",
    "#          JM_results_05['ndcg_cut_10']['all'],\n",
    "#          JM_results_06['ndcg_cut_10']['all'],\n",
    "#          JM_results_07['ndcg_cut_10']['all'],\n",
    "#          JM_results_08['ndcg_cut_10']['all'],\n",
    "#          JM_results_09['ndcg_cut_10']['all']]\n",
    "# plot_results(scores, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], \"lambda\")\n",
    "\n",
    "# Dirichlet Prior Plotting\n",
    "\n",
    "# DirichletPrior750 = partial(DirichletPrior, mu=750)\n",
    "# run_retrieval('DirichletPrior750', DirichletPrior750)\n",
    "# DirichletPrior1250 = partial(DirichletPrior, mu=1250)\n",
    "# run_retrieval('DirichletPrior1250', DirichletPrior1250)\n",
    "\n",
    "\n",
    "# Get all evaluation results.\n",
    "# DP_results_750 = evaluate(validation_set, \"DirichletPrior750\")\n",
    "# DP_results_1250 = evaluate(validation_set, \"DirichletPrior1250\")\n",
    "\n",
    "\n",
    "# scores = [DP_results_500['ndcg_cut_10']['all'],\n",
    "#          DP_results_750['ndcg_cut_10']['all'],\n",
    "#          DP_results_1000['ndcg_cut_10']['all'],\n",
    "#          DP_results_1250['ndcg_cut_10']['all'],\n",
    "#          DP_results_1500['ndcg_cut_10']['all']]\n",
    "# plot_results(scores, [500, 750, 1000, 1250, 1500], \"mu\")\n",
    "\n",
    "\n",
    "\n",
    "# # Absolute discounting plotting\n",
    "# AbsoluteDiscounting02 = partial(AbsoluteDiscounting, delta = 0.2)\n",
    "# run_retrieval('AbsoluteDiscounting02', AbsoluteDiscounting02)\n",
    "# AbsoluteDiscounting03 = partial(AbsoluteDiscounting, delta = 0.3)\n",
    "# run_retrieval('AbsoluteDiscounting03', AbsoluteDiscounting03)\n",
    "# AbsoluteDiscounting04 = partial(AbsoluteDiscounting, delta = 0.4)\n",
    "# run_retrieval('AbsoluteDiscounting04', AbsoluteDiscounting04)\n",
    "# AbsoluteDiscounting06 = partial(AbsoluteDiscounting, delta = 0.6)\n",
    "# run_retrieval('AbsoluteDiscounting06', AbsoluteDiscounting06)\n",
    "# AbsoluteDiscounting07 = partial(AbsoluteDiscounting, delta = 0.7)\n",
    "# run_retrieval('AbsoluteDiscounting07', AbsoluteDiscounting07)\n",
    "# AbsoluteDiscounting08 = partial(AbsoluteDiscounting, delta = 0.8)\n",
    "# run_retrieval('AbsoluteDiscounting08', AbsoluteDiscounting08)\n",
    "\n",
    "\n",
    "\n",
    "# # Get all evaluation results.\n",
    "# AD_results_02 = evaluate(validation_set, \"AbsoluteDiscounting02\")\n",
    "# AD_results_03 = evaluate(validation_set, \"AbsoluteDiscounting03\")\n",
    "# AD_results_04 = evaluate(validation_set, \"AbsoluteDiscounting04\")\n",
    "# AD_results_06 = evaluate(validation_set, \"AbsoluteDiscounting06\")\n",
    "# AD_results_07 = evaluate(validation_set, \"AbsoluteDiscounting07\")\n",
    "# AD_results_08 = evaluate(validation_set, \"AbsoluteDiscounting08\")\n",
    "\n",
    "# scores = [AD_results_01['ndcg_cut_10']['all'],\n",
    "#          AD_results_02['ndcg_cut_10']['all'],\n",
    "#          AD_results_03['ndcg_cut_10']['all'],\n",
    "#          AD_results_04['ndcg_cut_10']['all'],\n",
    "#          AD_results_05['ndcg_cut_10']['all'],\n",
    "#          AD_results_06['ndcg_cut_10']['all'],\n",
    "#          AD_results_07['ndcg_cut_10']['all'],\n",
    "#          AD_results_08['ndcg_cut_10']['all'],\n",
    "#          AD_results_09['ndcg_cut_10']['all']]\n",
    "# plot_results(scores, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], \"delta\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
